# NorCal-Coffee-Subscription
The purpose of this project is to find the best price (cost per lb for bulk coffee that is roasted in Northern California, defined by an imaginary line at the furthest south street in Santa Cruz, CA to the Oregon border at the North end, outlined by the Pacific Ocean to the West and the Nevada State Line to the East.) The overall purpose is to purchase coffee in bulk for my family to drink on a daily basis, but the coffee needs to be of excellent quality, so we will stick to only local roasters of the Northern California Area (as defined above). A local roaster is defined by a roastery that has less than 10 storefronts, all of which must be contained in Northern California (as outlined above).

A couple of the Roaster's that I used and collected data for this project do not fit the criteria but I made an exception for them. They are Verve and Blue Bottle they both have more than 10 storefronts.

I wanted to do this project for a couple reasons. I wanted to see what the best price I could get a premium coffee that is local to me was and I wanted to develop a project for my portfolio. I have drank all of the coffees that I used in this project and I went and found all the data myself to then add it to an Excel Spreadsheet. I then started figuring out what metrics I needed to look at to answer the following questions:

  - What is the best price per lb of premium coffee that is from a Northern California Roaster?
  - What would the overall cost incurred be to subscribe to the subsciption model that I decide to go with?
  - What would the cost be when factored down to each cup brewed in my personal Chemex for the subscription model I choose?
  
I was able to answer all of these questions and more with the data that I came up with. For this project I used Excel, Sheets, Docs, Word, SQL, Tableau, and R. I was able to analyze it using spreadsheets and organize it so I could then use it in SQL and R as well as make some visuals to represent my findings. I was able to use SQL to rework my dataset to make it easier to read and able to answer all three of my questions. Using SQL made my data look a lot cleaner once I had cleaned and validated it. Then I was able to take my data over to Tableau to create some nice looking visualizations. Lastly, I used R much like I used SQL to manipulte the data and clean it for analysis and I could easily answer all three questions with R. I was also able to create a viz with R and it is now posted in the Issues section of the project. I had a lot of fun doing this project and getting to further practice the skills that I have learned throughout the Google Data Analytics Certificate Program. I will keep adding my work files to the Issues section of the project.


- To see the original data you can download a copy in Excel here: Coffee Subscription Project.xlxs 
- For the work in R see: NorCal-Coffee.md
- To see the visuals from Tableau see: Tableau Vizzes.md
- For files used in the project see: NorCal-Coffee_files
